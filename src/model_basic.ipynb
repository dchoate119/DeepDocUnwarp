{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca3a43a5-b7dd-4131-9ae3-48f5d275b7f0",
   "metadata": {},
   "source": [
    "# Document aligner project\n",
    "## Basic Model Implementation\n",
    "- Working on milestone 1\n",
    "- Implementing a simple encoder decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9372fa8-a23f-423a-b62e-8ad6492b6aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_loader import get_dataloaders, visualize_batch\n",
    "from reconstruction_model import DocumentReconstructionModel\n",
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from typing import Dict, List, Tuple, Optional, Callable\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6389273-ed94-4d60-a5f7-f150ad3917cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2486 samples in /home/daniel-choate/Datasets/DocUnwarp/renders/synthetic_data_pitch_sweep\n",
      "Train samples: 1988, Val samples: 498\n"
     ]
    }
   ],
   "source": [
    "# Create dataloaders\n",
    "train_loader, val_loader = get_dataloaders(\n",
    "    data_dir='/home/daniel-choate/Datasets/DocUnwarp/renders/synthetic_data_pitch_sweep',\n",
    "    batch_size=8,\n",
    "    train_split=0.8,\n",
    "    img_size=(512, 512), #(256, 256)\n",
    ")\n",
    "\n",
    "# Visualize samples\n",
    "sample_batch = next(iter(train_loader))\n",
    "# visualize_batch(sample_batch, num_samples=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4803f3-313c-4604-898e-bf5b7d488fbe",
   "metadata": {},
   "source": [
    "# Model description\n",
    "- TODO\n",
    "\n",
    "Training:\n",
    "- [ ] Additional metrics (PSNR, SSIM)\n",
    "- [ ] Learning rate scheduling\n",
    "- [ ] Gradient clipping\n",
    "- [ ] Mixed precision training\n",
    "- [ ] Logging to tensorboard/wandb\n",
    "\n",
    "Main training loop\n",
    "- [ ] Implement better model architecture\n",
    "- [ ] Try different loss functions\n",
    "- [ ] Add learning rate scheduling\n",
    "- [ ] Implement early stopping\n",
    "- [ ] Add visualization and logging\n",
    "- [ ] Experiment with data augmentation\n",
    "- [ ] Use pretrained model from HuggingFace\n",
    "- [ ] Enable MASKED LOSSES\n",
    "- [ ] Use DEPTH\n",
    "- [ ] Use UV\n",
    "- [ ] Use BORDER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2004eaa2-e376-4dad-aef7-2742f4375bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training loop \n",
    "\n",
    "# def train_one_epoch(\n",
    "#     model: nn.Module, \n",
    "#     dataloader: DataLoader,\n",
    "#     criterion: nn.Module,\n",
    "#     optimizer: torch.optim.Optimizer, \n",
    "#     devie: torch.device, \n",
    "#     epoch: int\n",
    "# ) -> float:\n",
    "#     \"\"\" \n",
    "#     Train for one epoch\n",
    "\n",
    "#     *** CHECK TODO LIST ***\n",
    "#     \"\"\"\n",
    "#     model.train()\n",
    "#     total_loss = 0.0\n",
    "\n",
    "#     # Loop through each batch \n",
    "#     for batch_idx, batch in enumerate(dataloader):\n",
    "#         # Move data to device \n",
    "#         rgb = batch['rgb'].to(device)\n",
    "#         ground_truth = batch['ground_truth'].to(device)\n",
    "\n",
    "#         # Load mask if using masked loss\n",
    "#         mask = batch.get('border', None)\n",
    "#         if mask is not None:\n",
    "#             mask = mask.to(device)\n",
    "\n",
    "#         # Forward pass\n",
    "#         optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368f2c8e-f271-43ab-bae7-762268d4b58b",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e41ba305-0351-4b61-b3e4-75c4b9e7f588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop \n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main training loop \n",
    "\n",
    "    *** CHECK TODO LIST ***\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    DATA_DIR = '/home/daniel-choate/Datasets/DocUnwarp/renders/synthetic_data_pitch_sweep'\n",
    "    BATCH_SIZE = 8\n",
    "    NUM_EPOCHS = 50\n",
    "    LEARNING_RATE = 1e-4\n",
    "    IMG_SIZE = (512, 512)\n",
    "\n",
    "    # Set device \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader, val_loader = get_dataloaders(\n",
    "        data_dir=DATA_DIR,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        img_size=IMG_SIZE,\n",
    "        use_depth=False, # TODO: True if using depth info\n",
    "        use_uv=False, # TODO: True if using UV maps\n",
    "        use_border=False # TODO: True if using border masks for better training\n",
    "    )\n",
    "\n",
    "    # Visualize a batch (testing)\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    print(f\"Batch RGB shape: {sample_batch['rgb'].shape}\")\n",
    "    print(f\"Batch GT shape: {sample_batch['ground_truth'].shape}\")\n",
    "    if 'border' in sample_batch:\n",
    "        print(f\"Batch border mask shape: {sample_batch['border'].shape}\")\n",
    "    # visualize_batch(sample_batch) # Troubleshooting / sanity check \n",
    "\n",
    "\n",
    "    # Create model\n",
    "    model = DocumentReconstructionModel().to(device)\n",
    "    print(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "    # TODO: TRY DIFFERENT LOSS FUNCTIONS\n",
    "    # Option 1: simple losses (baseline, just to get it working)\n",
    "    criterion = nn.MSELoss() # Basic L2 Loss - sensitive to lighting\n",
    "    # criterion = nn.L1Loss() # L1 loss also sensitive to lighting\n",
    "\n",
    "    # Option 2: SSIM loss (RECOMMENDED - structure instead of lighting)\n",
    "    # criterion = SSIMLoss() # Might need a pip install\n",
    "\n",
    "    # Option 3: Masked losses (doc pixel focus)\n",
    "    # Make sure to do use_border = True above **\n",
    "    criterion = MaskedL1Loss(use_mask=True)\n",
    "    criterion = MaskedMSELoss(use_mask=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96cee6e9-4750-42eb-985b-db6e41b1bdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Reconstruction Dataset Loader\n",
      "==================================================\n",
      "Found 2486 samples in /home/daniel-choate/Datasets/DocUnwarp/renders/synthetic_data_pitch_sweep\n",
      "Train samples: 1988, Val samples: 498\n",
      "\n",
      "Dataset loaded successfully!\n",
      "Using device: cuda\n",
      "Found 2486 samples in /home/daniel-choate/Datasets/DocUnwarp/renders/synthetic_data_pitch_sweep\n",
      "Train samples: 1988, Val samples: 498\n",
      "Batch RGB shape: torch.Size([8, 3, 512, 512])\n",
      "Batch GT shape: torch.Size([8, 3, 512, 512])\n",
      "\n",
      "Model parameters: 483,267\n"
     ]
    }
   ],
   "source": [
    "# Execute main \n",
    "\n",
    "print(\"Document Reconstruction Dataset Loader\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Quick test \n",
    "train_loader, val_loader = get_dataloaders(\n",
    "    data_dir = '/home/daniel-choate/Datasets/DocUnwarp/renders/synthetic_data_pitch_sweep',\n",
    "    batch_size = 4, \n",
    "    img_size=(512, 512)\n",
    ")\n",
    "\n",
    "print(\"\\nDataset loaded successfully!\")\n",
    "\n",
    "# # Visualize sample batch \n",
    "# print(\"\\nVisualizing a sample batch ...\")\n",
    "# sample_batch = next(iter(train_loader))\n",
    "# print(f\"Batch shape - RGB: {sample_batch['rgb'].shape}, Ground Truth: {sample_batch['ground_truth'].shape}\")\n",
    "# visualize_batch(sample_batch, num_samples=min(4, sample_batch['rgb'].shape[0]))\n",
    "\n",
    "# Main training loop \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ccc8acc-ec19-4ef2-bfdd-67efee5dfc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Reconstruction Dataset Loader\n",
      "==================================================\n",
      "Found 2486 samples in /home/daniel-choate/Datasets/DocUnwarp/renders/synthetic_data_pitch_sweep\n",
      "Train samples: 1988, Val samples: 498\n",
      "\n",
      "Dataset loaded successfully!\n",
      "Using device: cuda\n",
      "Found 2486 samples in /home/daniel-choate/Datasets/DocUnwarp/renders/synthetic_data_pitch_sweep\n",
      "Train samples: 1988, Val samples: 498\n",
      "Batch RGB shape: torch.Size([8, 3, 512, 512])\n",
      "Batch GT shape: torch.Size([8, 3, 512, 512])\n",
      "\n",
      "Error loaded dataset: name 'DocumentReconstructionModel' is not defined\n",
      "Check that data directory exists and contains required files\n"
     ]
    }
   ],
   "source": [
    "# Execute main \n",
    "\n",
    "print(\"Document Reconstruction Dataset Loader\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Quick test \n",
    "try: \n",
    "    train_loader, val_loader = get_dataloaders(\n",
    "        data_dir = '/home/daniel-choate/Datasets/DocUnwarp/renders/synthetic_data_pitch_sweep',\n",
    "        batch_size = 4, \n",
    "        img_size=(512, 512)\n",
    "    )\n",
    "\n",
    "    print(\"\\nDataset loaded successfully!\")\n",
    "\n",
    "    # # Visualize sample batch \n",
    "    # print(\"\\nVisualizing a sample batch ...\")\n",
    "    # sample_batch = next(iter(train_loader))\n",
    "    # print(f\"Batch shape - RGB: {sample_batch['rgb'].shape}, Ground Truth: {sample_batch['ground_truth'].shape}\")\n",
    "    # visualize_batch(sample_batch, num_samples=min(4, sample_batch['rgb'].shape[0]))\n",
    "\n",
    "    # Main training loop \n",
    "    main()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError loading dataset: {e}\")\n",
    "    print(\"Check that data directory exists and contains required files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6066e6e1-6104-47da-bbcb-fd7f67166c14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6728a9b3-eab3-439d-ac7e-79e8efc56788",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
