{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca3a43a5-b7dd-4131-9ae3-48f5d275b7f0",
   "metadata": {},
   "source": [
    "# Document aligner project\n",
    "## NEW Model Implementation\n",
    "- U-Net with skip connections\n",
    "- Flow predictor, differentiable warping\n",
    "- Standard encoder-decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9372fa8-a23f-423a-b62e-8ad6492b6aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_loader import get_dataloaders, visualize_batch\n",
    "from reconstruction_model import *\n",
    "from reconstruction_model import MaskedL1Loss, MaskedMSELoss, SSIMLoss, UVReconstructionLoss\n",
    "from training_val_dewarpnet import train_one_epoch, validate\n",
    "\n",
    "import time \n",
    "import os\n",
    "import glob\n",
    "from typing import Dict, List, Tuple, Optional, Callable\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6389273-ed94-4d60-a5f7-f150ad3917cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_loader, val_loader = get_dataloaders(\n",
    "    data_dir='/home/daniel-choate/Datasets/DocUnwarp/renders/synthetic_data_pitch_sweep',\n",
    "    batch_size=8,\n",
    "    train_split=0.8,\n",
    "    img_size=(512, 512), #(256, 256)\n",
    ")\n",
    "\n",
    "# Visualize samples\n",
    "sample_batch = next(iter(train_loader))\n",
    "# visualize_batch(sample_batch, num_samples=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4803f3-313c-4604-898e-bf5b7d488fbe",
   "metadata": {},
   "source": [
    "# Model description\n",
    "- TODO\n",
    "\n",
    "Training:\n",
    "- [ ] Additional metrics (PSNR, SSIM)\n",
    "- [ ] Learning rate scheduling\n",
    "- [ ] Gradient clipping\n",
    "- [ ] Mixed precision training\n",
    "- [ ] Logging to tensorboard/wandb\n",
    "\n",
    "Main training loop\n",
    "- [ ] Implement better model architecture\n",
    "- [ ] Try different loss functions\n",
    "- [ ] Add learning rate scheduling\n",
    "- [ ] Implement early stopping\n",
    "- [ ] Add visualization and logging\n",
    "- [ ] Experiment with data augmentation\n",
    "- [ ] Use pretrained model from HuggingFace\n",
    "- [ ] Enable MASKED LOSSES\n",
    "- [ ] Use DEPTH\n",
    "- [ ] Use UV\n",
    "- [ ] Use BORDER\n",
    "- [ ] Try different OPTIMIZERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368f2c8e-f271-43ab-bae7-762268d4b58b",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41ba305-0351-4b61-b3e4-75c4b9e7f588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop \n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main training loop \n",
    "\n",
    "    *** CHECK TODO LIST ***\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    DATA_DIR = '/home/daniel-choate/Datasets/DocUnwarp/renders/synthetic_data_pitch_sweep'\n",
    "    BATCH_SIZE = 8\n",
    "    NUM_EPOCHS = 50\n",
    "    LEARNING_RATE = 1e-4\n",
    "    IMG_SIZE = (256, 256) # (512, 512) Using smaller images for faster training for now\n",
    "\n",
    "    # Set device \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader, val_loader = get_dataloaders(\n",
    "        data_dir=DATA_DIR,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        img_size=IMG_SIZE,\n",
    "        use_depth=False, # TODO: True if using depth info\n",
    "        use_uv=True, # TODO: True if using UV maps\n",
    "        use_border=True # TODO: True if using border masks for better training\n",
    "    )\n",
    "\n",
    "    # Visualize a batch (testing)\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    print(f\"Batch RGB shape: {sample_batch['rgb'].shape}\")\n",
    "    print(f\"Batch GT coords: {sample_batch['coords'].shape}\")\n",
    "    print(f\"Batch GT backward map: {sample_batch['uv'].shape}\")\n",
    "    if 'border' in sample_batch:\n",
    "        print(f\"Batch border mask shape: {sample_batch['border'].shape}\")\n",
    "    # visualize_batch(sample_batch) # Troubleshooting / sanity check \n",
    "\n",
    "\n",
    "    # Create model\n",
    "    model = DewarpNet().to(device)\n",
    "    print(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "\n",
    "    # TODO: TRY DIFFERENT OPTIMIZERS\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "\n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        # Epoch timing check\n",
    "        start_epoch = time.time()\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"EPOCH {epoch+1}/{NUM_EPOCHS}\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        # TRAIN \n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            rgb = batch['rgb'].to(device)\n",
    "            C_gt = batch['coords'].to(device)\n",
    "            B_gt = batch['uv'].to(device)\n",
    "            D_gt = batch['ground_truth'].to(device)\n",
    "            mask = batch.get('border', None)\n",
    "            if mask is not None:\n",
    "                mask = mask.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            D_hat, C_hat, B_hat = model(rgb)\n",
    "\n",
    "            losses = compute_loss(\n",
    "                C_hat=C_hat, C=C_gt,\n",
    "                B_hat=B_hat, B=B_gt,\n",
    "                D_hat=D_hat, D=D_gt,\n",
    "                mask=mask,\n",
    "                alpha=1.0,\n",
    "                beta=1.0,\n",
    "                lambda_grad=0.1,\n",
    "                gamma=1.0,\n",
    "                delta=1.0\n",
    "            )\n",
    "            loss = losses['total']\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # --- VALIDATE ---\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                rgb = batch['rgb'].to(device)\n",
    "                C_gt = batch['coords'].to(device)\n",
    "                B_gt = batch['uv'].to(device)\n",
    "                D_gt = batch['ground_truth'].to(device)\n",
    "                mask = batch.get('border', None)\n",
    "                if mask is not None:\n",
    "                    mask = mask.to(device)\n",
    "\n",
    "                D_hat, C_hat, B_hat = model(rgb)\n",
    "                losses = compute_loss(\n",
    "                    C_hat=C_hat, C=C_gt,\n",
    "                    B_hat=B_hat, B=B_gt,\n",
    "                    D_hat=D_hat, D=D_gt,\n",
    "                    mask=mask\n",
    "                )\n",
    "                total_val_loss += losses['total'].item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Save the best model \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_dewarpnet_model.pth')\n",
    "            print(f\"Saved best model with val loss {val_loss:.4f}\")\n",
    "\n",
    "        epoch_time = time.time() - start_epoch\n",
    "        print(f\"Epoch time: {epoch_time:.4f}s\")\n",
    "\n",
    "    print(\"\\nTraining complete!\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "\n",
    "    # Loss curve \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    epochs = range(1, NUM_EPOCHS + 1)\n",
    "    plt.plot(epochs, train_losses, label='Train Loss')\n",
    "    plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('DewarpNet Training & Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    sample_rgb = sample_batch['rgb'].to(device)\n",
    "    with torch.no_grad():\n",
    "        D_hat, _, _ = model(sample_rgb)\n",
    "    img = D_hat[0].cpu().permute(1, 2, 0).numpy()\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(img)\n",
    "    plt.title('Example Predicted Unwarped Image')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cee6e9-4750-42eb-985b-db6e41b1bdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute main \n",
    "\n",
    "print(\"Document Reconstruction Dataset Loader\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Quick test \n",
    "train_loader, val_loader = get_dataloaders(\n",
    "    data_dir = '/home/daniel-choate/Datasets/DocUnwarp/renders/synthetic_data_pitch_sweep',\n",
    "    batch_size = 4, \n",
    "    img_size=(256, 256) # (512,512)\n",
    ")\n",
    "\n",
    "print(\"\\nDataset loaded successfully!\")\n",
    "\n",
    "# # Visualize sample batch \n",
    "# print(\"\\nVisualizing a sample batch ...\")\n",
    "# sample_batch = next(iter(train_loader))\n",
    "# print(f\"Batch shape - RGB: {sample_batch['rgb'].shape}, Ground Truth: {sample_batch['ground_truth'].shape}\")\n",
    "# visualize_batch(sample_batch, num_samples=min(4, sample_batch['rgb'].shape[0]))\n",
    "\n",
    "# Main training loop \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccc8acc-ec19-4ef2-bfdd-67efee5dfc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss plot \n",
    "\n",
    "\n",
    "history = torch.load('training_history.pth')\n",
    "\n",
    "train_losses = history['train_losses']\n",
    "val_losses = history['val_losses']\n",
    "epochs = range(1, 50 + 1)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(epochs, train_losses, label='Train Loss')\n",
    "plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training & Validation Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6066e6e1-6104-47da-bbcb-fd7f67166c14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6728a9b3-eab3-439d-ac7e-79e8efc56788",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
